{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Random tensor on device: tensor([[0.0358, 0.6989, 0.2847, 0.6577, 0.4707],\n",
      "        [0.5551, 0.4569, 0.6650, 0.7052, 0.6153],\n",
      "        [0.9382, 0.4860, 0.5263, 0.8036, 0.2150],\n",
      "        [0.1697, 0.7612, 0.6752, 0.4719, 0.1756],\n",
      "        [0.4485, 0.2796, 0.3450, 0.4040, 0.8787]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 检查 GPU 是否可用\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# 创建一个随机张量并移动到 GPU（如果可用）\n",
    "x = torch.rand(5, 5).to(device)\n",
    "print(\"Random tensor on device:\", x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ChatTTS.core:Load from cache: C:\\Users\\23668/.cache/huggingface\\hub/models--2Noise--ChatTTS/snapshots\\1a3c04a8b0651689bd9242fbb55b1f4b5a9aef84\n",
      "INFO:ChatTTS.core:use cuda:0\n",
      "g:\\App\\Anaconda\\envs\\pytorch-gpu\\Lib\\site-packages\\ChatTTS\\core.py:175: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  vocos.load_state_dict(torch.load(vocos_ckpt_path, map_location=device))\n",
      "INFO:ChatTTS.core:vocos loaded.\n",
      "g:\\App\\Anaconda\\envs\\pytorch-gpu\\Lib\\site-packages\\ChatTTS\\core.py:183: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  dvae.load_state_dict(torch.load(dvae_ckpt_path, map_location=device))\n",
      "INFO:ChatTTS.core:dvae loaded.\n",
      "g:\\App\\Anaconda\\envs\\pytorch-gpu\\Lib\\site-packages\\ChatTTS\\core.py:191: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  gpt.load_state_dict(torch.load(gpt_ckpt_path, map_location=\"cpu\"))\n",
      "g:\\App\\Anaconda\\envs\\pytorch-gpu\\Lib\\site-packages\\ChatTTS\\core.py:201: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.pretrain_models[\"spk_stat\"] = torch.load(spk_stat_path).to(device)\n",
      "INFO:ChatTTS.core:gpt loaded.\n",
      "g:\\App\\Anaconda\\envs\\pytorch-gpu\\Lib\\site-packages\\ChatTTS\\core.py:208: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  decoder.load_state_dict(torch.load(decoder_ckpt_path, map_location=device))\n",
      "INFO:ChatTTS.core:decoder loaded.\n",
      "g:\\App\\Anaconda\\envs\\pytorch-gpu\\Lib\\site-packages\\ChatTTS\\core.py:213: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  tokenizer = torch.load(tokenizer_path, map_location=device)\n",
      "INFO:ChatTTS.core:tokenizer loaded.\n",
      "INFO:ChatTTS.core:All initialized.\n",
      "C:\\Users\\23668\\AppData\\Local\\Temp\\ipykernel_22988\\858400544.py:30: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  std , mean = torch.load(f\"{MODELPATH}/spk_stat.pt\").chunk(2)\n",
      "INFO:ChatTTS.core:All initialized.\n",
      " 28%|██▊       | 106/384 [00:04<00:11, 24.19it/s]\n",
      " 51%|█████     | 1035/2048 [00:41<00:40, 24.72it/s]\n"
     ]
    }
   ],
   "source": [
    "import ChatTTS\n",
    "import torch\n",
    "import torchaudio\n",
    "import soundfile\n",
    "import time\n",
    "import torch\n",
    "import random\n",
    "\n",
    "module_name = \"chatutil\"\n",
    "\n",
    "\n",
    "MODELPATH = \"models/pzc163/chatTTS/asset\"\n",
    " \n",
    "class ChatTTSUtil:\n",
    "    def __init__(self ,\n",
    "                 modelPath = MODELPATH,\n",
    "                 saveFilePath = \"output/\" ,\n",
    "                 fixSpkStyle = True):\n",
    "        # 初始化ChatTTSUtil类，设置模型路径、保存文件路径和是否固定说话风格\n",
    "        self.modelPath = modelPath\n",
    "        self.wavfilePath = saveFilePath\n",
    "        self.fixSpkStyle = fixSpkStyle\n",
    "        self.chat = ChatTTS.Chat()\n",
    "        self.chat.load_models(local_path = modelPath)\n",
    "        # 设置文本精炼参数\n",
    "        self.params_refine_text = {\"prompt\": \"[oral_0][laugh_0][break_0]\"}\n",
    "        # Config the speech style with random generation\n",
    "        std , mean = torch.load(f\"{MODELPATH}/spk_stat.pt\").chunk(2)\n",
    "        rand_spk = torch.randn(768) * std + mean\n",
    "        self.params_infer_code = {\n",
    "            \"spk_emb\": rand_spk,\n",
    "            \"temperature\": .3,\n",
    "            \"top_P\": 0.7,\n",
    "            \"top_K\": 20,\n",
    "            \"prompt\": \"[speed_5]\"\n",
    "        }\n",
    "\n",
    "    def setRefineTextConf(self , oralConf = \"[oral_0]\" , laughConf = \"[laugh_0]\" , breakConf = \"[break_0]\"):\n",
    "        # 定义一个方法setRefineTextConf，用于设置文本精炼的配置\n",
    "        # 参数oralConf默认值为\"[oral_0]\"，表示口语化配置\n",
    "        # 参数laughConf默认值为\"[laugh_0]\"，表示笑声配置\n",
    "        # 参数breakConf默认值为\"[break_0]\"，表示中断配置\n",
    "        self.params_refine_text = {\"prompt\": f\"{oralConf}{laughConf}{breakConf}\"}\n",
    "\n",
    "    def setInferCode(self , temperature = 0.3 , top_P = 0.7 , top_K = 20 , speed = \"[speed_5]\"):\n",
    "        # 设置推理代码的参数\n",
    "        # temperature: 控制生成文本的随机性，值越大，生成的文本越随机\n",
    "        self.params_infer_code[\"temperature\"] = temperature\n",
    "        # top_P: 控制生成文本的多样性，值越大，生成的文本越多样\n",
    "        self.params_infer_code[\"top_P\"] = top_P\n",
    "        # top_K: 控制生成文本的词汇量，值越大，生成的文本使用的词汇越多\n",
    "        self.params_infer_code[\"top_K\"] = top_K\n",
    "        # speed: 控制生成文本的速度，这里使用了一个字符串表示速度等级\n",
    "        self.params_infer_code[\"prompt\"] = speed\n",
    "\n",
    "    def generateSound(self , texts , savePath = \"output/\" , filePrefix = \"output\"):\n",
    "        # 调用chat对象的infer方法，将文本转换为音频波形\n",
    "        # texts: 要转换为音频的文本列表\n",
    "        # use_decoder: 是否使用解码器\n",
    "        # params_refine_text: 文本精炼参数\n",
    "        # params_infer_code: 音频生成参数\n",
    "        wavs = self.chat.infer(texts , use_decoder = True , params_refine_text = self.params_refine_text , params_infer_code = self.params_infer_code)\n",
    "        # 初始化一个空列表，用于存储生成的音频文件路径\n",
    "        wavFilePath = []\n",
    "        # 遍历生成的音频波形列表\n",
    "        for (index, wave) in enumerate(wavs):\n",
    "            # 使用soundfile库将音频波形写入文件\n",
    "            # 文件路径由savePath、filePrefix和索引组成\n",
    "            # wave[0]表示音频数据，24000是采样率\n",
    "            soundfile.write(f\"{savePath}{filePrefix}{index}.wav\" , wave[0] , 24000)\n",
    "            # 将生成的音频文件路径添加到列表中\n",
    "            wavFilePath.append(f\"{savePath}{filePrefix}{index}.wav\")\n",
    "        # 返回生成的音频文件路径列表\n",
    "        return wavFilePath\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    chUtil = ChatTTSUtil()\n",
    "    texts = [\n",
    "        \"大家好，我是Chat T T S，欢迎来到畅的科技工坊。\",\n",
    "        \"太棒了，我竟然是第一位嘉宾。\",\n",
    "        \"我是Chat T T S， 是专门为对话场景设计的文本转语音模型，例如大语言助手对话任务。我支持英文和中文两种语言。最大的模型使用了10万小时以上的中英文数据进行训练。目前在huggingface中的开源版本为4万小时训练且未S F T 的版本。\",\n",
    "    \"耶，我们开始吧\"\n",
    "    ]\n",
    "    chUtil.setInferCode(0.8 , 0.7 , 20 , speed = \"[speed_5]\")\n",
    "    chUtil.generateSound(texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5000\n",
      " * Running on http://172.20.3.13:5000\n",
      "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask\n",
    "import ChatTTS\n",
    "import torch\n",
    "import torchaudio\n",
    "import soundfile\n",
    "import time\n",
    "import torch\n",
    "import random\n",
    "\n",
    "\n",
    "module_name = \"chatutil\"\n",
    "\n",
    "MODELPATH = \"models/pzc163/chatTTS/asset\"\n",
    "\n",
    "app = Flask(__name__)\n",
    " \n",
    "class ChatTTSUtil:\n",
    "    def __init__(self ,\n",
    "                 modelPath = MODELPATH,\n",
    "                 saveFilePath = \"output/\" ,\n",
    "                 fixSpkStyle = True):\n",
    "        # 初始化ChatTTSUtil类，设置模型路径、保存文件路径和是否固定说话风格\n",
    "        self.modelPath = modelPath\n",
    "        self.wavfilePath = saveFilePath\n",
    "        self.fixSpkStyle = fixSpkStyle\n",
    "        self.chat = ChatTTS.Chat()\n",
    "        self.chat.load_models(local_path = modelPath)\n",
    "        # 设置文本精炼参数\n",
    "        self.params_refine_text = {\"prompt\": \"[oral_0][laugh_0][break_0]\"}\n",
    "        # Config the speech style with random generation\n",
    "        std , mean = torch.load(f\"{MODELPATH}/spk_stat.pt\").chunk(2)\n",
    "        rand_spk = torch.randn(768) * std + mean\n",
    "        self.params_infer_code = {\n",
    "            \"spk_emb\": rand_spk,\n",
    "            \"temperature\": .3,\n",
    "            \"top_P\": 0.7,\n",
    "            \"top_K\": 20,\n",
    "            \"prompt\": \"[speed_5]\"\n",
    "        }\n",
    "\n",
    "    def setRefineTextConf(self , oralConf = \"[oral_0]\" , laughConf = \"[laugh_0]\" , breakConf = \"[break_0]\"):\n",
    "        # 定义一个方法setRefineTextConf，用于设置文本精炼的配置\n",
    "        # 参数oralConf默认值为\"[oral_0]\"，表示口语化配置\n",
    "        # 参数laughConf默认值为\"[laugh_0]\"，表示笑声配置\n",
    "        # 参数breakConf默认值为\"[break_0]\"，表示中断配置\n",
    "        self.params_refine_text = {\"prompt\": f\"{oralConf}{laughConf}{breakConf}\"}\n",
    "\n",
    "    def setInferCode(self , temperature = 0.3 , top_P = 0.7 , top_K = 20 , speed = \"[speed_5]\"):\n",
    "        # 设置推理代码的参数\n",
    "        # temperature: 控制生成文本的随机性，值越大，生成的文本越随机\n",
    "        self.params_infer_code[\"temperature\"] = temperature\n",
    "        # top_P: 控制生成文本的多样性，值越大，生成的文本越多样\n",
    "        self.params_infer_code[\"top_P\"] = top_P\n",
    "        # top_K: 控制生成文本的词汇量，值越大，生成的文本使用的词汇越多\n",
    "        self.params_infer_code[\"top_K\"] = top_K\n",
    "        # speed: 控制生成文本的速度，这里使用了一个字符串表示速度等级\n",
    "        self.params_infer_code[\"prompt\"] = speed\n",
    "\n",
    "    def generateSound(self , texts , savePath = \"output/\" , filePrefix = \"output\"):\n",
    "        # 调用chat对象的infer方法，将文本转换为音频波形\n",
    "        # texts: 要转换为音频的文本列表\n",
    "        # use_decoder: 是否使用解码器\n",
    "        # params_refine_text: 文本精炼参数\n",
    "        # params_infer_code: 音频生成参数\n",
    "        wavs = self.chat.infer(texts , use_decoder = True , params_refine_text = self.params_refine_text , params_infer_code = self.params_infer_code)\n",
    "        # 初始化一个空列表，用于存储生成的音频文件路径\n",
    "        wavFilePath = []\n",
    "        # 遍历生成的音频波形列表\n",
    "        for (index, wave) in enumerate(wavs):\n",
    "            # 使用soundfile库将音频波形写入文件\n",
    "            # 文件路径由savePath、filePrefix和索引组成\n",
    "            # wave[0]表示音频数据，24000是采样率\n",
    "            soundfile.write(f\"{savePath}{filePrefix}{index}.wav\" , wave[0] , 24000)\n",
    "            # 将生成的音频文件路径添加到列表中\n",
    "            wavFilePath.append(f\"{savePath}{filePrefix}{index}.wav\")\n",
    "        # 返回生成的音频文件路径列表\n",
    "        return wavFilePath\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "@app.route('/chat_out', methods=['POST', 'GET'])\n",
    "def chat_out():\n",
    "    chUtil = ChatTTSUtil()\n",
    "    texts = [\n",
    "        \"大家好，我是Chat T T S，欢迎来到畅的科技工坊。\",\n",
    "        \"太棒了，我竟然是第一位嘉宾。\",\n",
    "        \"我是Chat T T S， 是专门为对话场景设计的文本转语音模型，例如大语言助手对话任务。我支持英文和中文两种语言。最大的模型使用了10万小时以上的中英文数据进行训练。目前在huggingface中的开源版本为4万小时训练且未S F T 的版本。\",\n",
    "    \"耶，我们开始吧\"\n",
    "    ]\n",
    "    out_chat = []\n",
    "    chUtil.setInferCode(0.8 , 0.7 , 20 , speed = \"[speed_3]\")\n",
    "    out_chat = chUtil.generateSound(texts)\n",
    "    return out_chat\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(host='0.0.0.0', port=5000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "path in endpoint is not allowed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError occurred:\u001b[39m\u001b[38;5;124m\"\u001b[39m, err)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# 配置 MinIO 客户端\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m client \u001b[38;5;241m=\u001b[39m \u001b[43mMinio\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttp://47.108.214.25:9000/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccess_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mminioadmin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43msecret_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mminioadmin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43msecure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m     18\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# 设置参数\u001b[39;00m\n\u001b[0;32m     21\u001b[0m bucket_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudios\u001b[39m\u001b[38;5;124m\"\u001b[39m        \u001b[38;5;66;03m# 存储桶名称\u001b[39;00m\n",
      "File \u001b[1;32mg:\\App\\Anaconda\\envs\\pytorch-gpu\\Lib\\site-packages\\minio\\api.py:153\u001b[0m, in \u001b[0;36mMinio.__init__\u001b[1;34m(self, endpoint, access_key, secret_key, session_token, secure, region, http_client, credentials, cert_check)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    149\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHTTP client should be instance of `urllib3.PoolManager`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    150\u001b[0m     )\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_region_map \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m--> 153\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_base_url \u001b[38;5;241m=\u001b[39m \u001b[43mBaseURL\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttps://\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msecure\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttp://\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mregion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_user_agent \u001b[38;5;241m=\u001b[39m _DEFAULT_USER_AGENT\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trace_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mg:\\App\\Anaconda\\envs\\pytorch-gpu\\Lib\\site-packages\\minio\\helpers.py:551\u001b[0m, in \u001b[0;36mBaseURL.__init__\u001b[1;34m(self, endpoint, region)\u001b[0m\n\u001b[0;32m    550\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, endpoint: \u001b[38;5;28mstr\u001b[39m, region: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m--> 551\u001b[0m     url \u001b[38;5;241m=\u001b[39m \u001b[43m_parse_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    553\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m region \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _REGION_REGEX\u001b[38;5;241m.\u001b[39mmatch(region):\n\u001b[0;32m    554\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minvalid region \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mregion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mg:\\App\\Anaconda\\envs\\pytorch-gpu\\Lib\\site-packages\\minio\\helpers.py:512\u001b[0m, in \u001b[0;36m_parse_url\u001b[1;34m(endpoint)\u001b[0m\n\u001b[0;32m    509\u001b[0m url \u001b[38;5;241m=\u001b[39m url_replace(url, scheme\u001b[38;5;241m=\u001b[39murl\u001b[38;5;241m.\u001b[39mscheme\u001b[38;5;241m.\u001b[39mlower())\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m url\u001b[38;5;241m.\u001b[39mpath \u001b[38;5;129;01mand\u001b[39;00m url\u001b[38;5;241m.\u001b[39mpath \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 512\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath in endpoint is not allowed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    514\u001b[0m url \u001b[38;5;241m=\u001b[39m url_replace(url, path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    516\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m url\u001b[38;5;241m.\u001b[39mquery:\n",
      "\u001b[1;31mValueError\u001b[0m: path in endpoint is not allowed"
     ]
    }
   ],
   "source": [
    "from minio import Minio\n",
    "from minio.error import S3Error\n",
    "\n",
    "def download_file(bucket_name, object_name, download_path):\n",
    "    try:\n",
    "        # 下载文件\n",
    "        client.fget_object(bucket_name, object_name, download_path)\n",
    "        print(f\"File {object_name} downloaded successfully to {download_path}\")\n",
    "    except S3Error as err:\n",
    "        print(\"Error occurred:\", err)\n",
    "\n",
    "# 配置 MinIO 客户端\n",
    "client = Minio(\n",
    "    endpoint=\"http://47.108.214.25:9000/\",\n",
    "    access_key=\"minioadmin\",\n",
    "    secret_key=\"minioadmin\",\n",
    "    secure=False\n",
    ")\n",
    "\n",
    "# 设置参数\n",
    "bucket_name = \"audios\"        # 存储桶名称\n",
    "object_name = \"2025/02/04/be7af6d7cc2a402397be24984a65f39a.flac\"      # 要下载的文件名\n",
    "download_path = \"output.mp3\"  # 本地保存路径\n",
    "\n",
    "# 调用下载函数\n",
    "download_file(bucket_name, object_name, download_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件已成功下载到 output.flac\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"http://47.108.214.25:9000/audios/2025/02/04/be7af6d7cc2a402397be24984a65f39a.flac\"\n",
    "output_path = \"output.flac\"  # 本地保存路径\n",
    "\n",
    "try:\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # 检查请求是否成功\n",
    "    with open(output_path, \"wb\") as file:\n",
    "        file.write(response.content)\n",
    "    print(f\"文件已成功下载到 {output_path}\")\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"下载失败：{e}\")\n",
    "    print(\"请检查 URL 的合法性，确保网络连接正常，并确认服务器是否可用。\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
